{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "\n",
    "In this notebook, I'll show you how to save and load models with PyTorch. This is important because you'll often want to load previously trained models to use in making predictions or to continue training on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "\n",
    "Here I'm just going to use the same model as in part 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
    "        ''' Builds a feedforward network with arbitrary hidden layers.\n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            input_size: integer, size of the input layer\n",
    "            output_size: integer, size of the output layer\n",
    "            hidden_layers: list of integers, the sizes of the hidden layers\n",
    "        \n",
    "        '''\n",
    "        super().__init__()\n",
    "        # Input to a hidden layer\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        # Add a variable number of more hidden layers\n",
    "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' Forward pass through the network, returns the output logits '''\n",
    "        \n",
    "        for each in self.hidden_layers:\n",
    "            x = F.relu(each(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network\n",
    "\n",
    "And training the network the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "model = Network(784, 10, [500, 100])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 100\n",
    "for e in range(epochs):\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        # Wrap images and labels in Variables so we can calculate gradients\n",
    "        inputs = Variable(images)\n",
    "        targets = Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            # Model in inference mode, dropout is off\n",
    "            model.eval()\n",
    "            \n",
    "            accuracy = 0\n",
    "            test_loss = 0\n",
    "            for ii, (images, labels) in enumerate(testloader):\n",
    "                \n",
    "                images = images.resize_(images.size()[0], 784)\n",
    "                # Set volatile to True so we don't save the history\n",
    "                inputs = Variable(images, volatile=True)\n",
    "                labels = Variable(labels, volatile=True)\n",
    "\n",
    "                output = model.forward(inputs)\n",
    "                test_loss += criterion(output, labels).data[0]\n",
    "                \n",
    "                ## Calculating the accuracy \n",
    "                # Model's output is log-softmax, take exponential to get the probabilities\n",
    "                ps = torch.exp(output).data\n",
    "                # Class with highest probability is our predicted class, compare with true label\n",
    "                equality = (labels.data == ps.max(1)[1])\n",
    "                # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
    "                accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
    "            \n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n",
    "            \n",
    "            running_loss = 0\n",
    "            \n",
    "            # Make sure dropout is on for training\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading networks\n",
    "\n",
    "As you can imagine, it's impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n",
    "\n",
    "The parameters for PyTorch networks are stored in a model's `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our network: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500)\n",
      "    (1): Linear(in_features=500, out_features=100)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `'checkpoint.pth'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load the state dict with `torch.load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to load the state dict in to the network, you do `model.load_state_dict(state_dict)'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty straightforward, but as usual it's a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "While copying the parameter named hidden_layers.0.weight, whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([500, 784]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m                     \u001b[0mown_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [400 x 784] and src [500 x 784] to have the same number of elements, but got 313600 and 392000 elements respectively at /Users/soumith/minicondabuild3/conda-bld/pytorch_1512381214802/work/torch/lib/TH/generic/THTensorCopy.c:86",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-74e14cc8e983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    485\u001b[0m                                        \u001b[0;34m'whose dimensions in the model are {} and '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                                        \u001b[0;34m'whose dimensions in the checkpoint are {}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                                        .format(name, own_state[name].size(), param.size()))\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 raise KeyError('unexpected key \"{}\" in state_dict'\n",
      "\u001b[0;31mRuntimeError\u001b[0m: While copying the parameter named hidden_layers.0.weight, whose dimensions in the model are torch.Size([400, 784]) and whose dimensions in the checkpoint are torch.Size([500, 784])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "net = Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = Network(checkpoint['input_size'],\n",
    "                    checkpoint['output_size'],\n",
    "                    checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=500)\n",
      "    (1): Linear(in_features=500, out_features=100)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
